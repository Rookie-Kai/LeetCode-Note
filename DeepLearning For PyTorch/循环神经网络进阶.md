# 深度循环神经网络
含有多个隐藏层的神经网络被称作深度循环神经网络。

每个隐藏状态不断传递至当前层的下一时间步和当前时间步的下一层

![](image\6.9_deep-rnn.svg)

第一隐藏层隐藏状态的计算：

$$H_t^{(1)} = \phi (X_tW_{xh}^{(1)}+H_{t-1}^{(1)}W_{hh}^{(1)}+b_h^{(1)})$$

同理，$1 \lt \ell \le L$时，第$\ell$隐藏层的计算：

$$H_t^{(\ell)} = \phi (H_t^{(\ell-1)}W_{xh}^{(\ell)}+H_{t-1}^{(\ell)}W_{hh}^{(\ell)}+b_h^{(\ell)})$$

最终，输出层的输出仅基于第$L$隐藏层的隐藏状态：

$$O_t = H_t^{(L)}W_{hq}+b_q$$

**注：** 隐藏层个数$L$和隐藏单元个数$h$都是超参数，如果将隐藏状态的计算换成门控循环单元或者长短期记忆的计算，我们可以得到深度门控循环神经网络。


# 双向循环神经网络
时间步不仅是由前面的较早时间步序列决定的，有时时间步也可能由后面时间步决定。双向循环神经网络便是通过增加从后往前传递信息的隐藏层来处理该类信息。

![](image\6.10_birnn.svg)

正向隐藏状态为$\overrightarrow{H_t}$（正向隐藏单元个数为$h$），反向隐藏状态为$\overleftarrow{H_t}$（反向隐藏单元个数为$h$），计算可得：

$$\overrightarrow{H_t} = \phi(X_tW_{xh}^{(f)}+\overrightarrow{H}_{t-1}W_{hh}^{(f)}+b_h^{(f)})$$

$$\overleftarrow{H_t} = \phi(X_tW_{xh}^{(b)}+\overleftarrow{H}_{t+1}W_{hh}^{(b)}+b_h^{(b)})$$

连结两个方向的隐藏状态$\overrightarrow{H_t} \in \mathbb{R}^{n \times h}$与$\overleftarrow{H_t} \in \mathbb{R}^{n \times h}$获得隐藏状态$H_t \in \mathbb{R}^{n \times 2h}$，将其输入到输出层计算输出为：

$$O_t = H_tW_{hq}+b_q$$