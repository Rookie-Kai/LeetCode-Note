# 池化层

池化层的作用是为了缓解卷积层对 **位置** 的过度敏感性

## 二维最大池化层与平均池化层

类似卷积层，池化层每次对输入数据的一个固定形状窗口（池化窗口）中的元素计算输出。

但卷积层里对输入与核进行互相关计算，而池化层直接计算池化窗口内元素的 **最大值或平均值**。

故该运算也分别叫做最大池化或平均池化。在二维最大池化中，池化窗口从输入数组的左上方开始，按从左到右，从上到下的顺序，依次在输入数组上滑动，当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。

![](image\5.4_pooling.svg)

上图中四个输出元素的计算如下：

$$max(0,1,3,4) = 4$$

$$max(1,2,4,5) = 5$$

$$max(3,4,6,7) = 7$$

$$max(4,5,7,8) = 8$$

二维平均池化层的工作原理与二维最大池化类似，将最大运算符换为平均运算符即可。

## 填充与步幅

类似卷积层，池化层也可在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制相同。

池化层的填充和步幅可被指定。

## 多通道

在处理多通道输入数据时，池化层对每个输入通道 **分别池化** ，而不是像卷积层那样将各个输入按通道相加。因此池化层的输出通道数与输入通道数相等。

# LeNet卷积神经网络

在通过含单隐藏层的多层感知机模型对Fashion-MNIST数据集中的图像进行分类时，我们即将图像（$28 \times 28$）中的像素逐行展开，获得长度为784的向量，输入到全连接层中。但是该方法的局限性在于：

- 图像在同一列邻近的像素在这个向量中可能相距较远，它们构成的模式可能难以被模型识别。
- 对于大尺寸的图像，使用全连接层容易造成模型过大，会带来过复杂的模型和过高的存储开销。

若采用卷积层：
 - 卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均有可能被有效识别
 - 卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。

 卷积神经网络：含有卷积层的网络

## LeNet模型

 LeNet的结构如下：

 ![](image\5.5_lenet.png)

 ![](image\q5ndwsmsao.png)

 leNet分为卷积层块和全连接层两个部分。

 卷积层块里的基本单位是卷积层后接平均（最大）池化层：
 - 卷积层用来识别图像里的空间模式，如线条和物体局部
 - 平均（最大）池化层用来降低卷积层对位置的敏感性
 
 卷积层块由以上两个基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5 \times 5$的窗口，在输出上使用$sigmoid$函数。
 
 第一个卷积层输出通道数为6，第二个卷积层的输出通道数增加到16，这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。

卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（通过flatten函数变为一维数组）。此时全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积

 全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。

因此。LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。

