# 优化
优化与深度学习的区别：
- 优化方法目标：降低训练集损失函数值
- 深度学习目标：降低测试集损失函数值（提高泛化性）

## 优化在深度学习中的挑战

1. 局部最小值
2. 鞍点
3. 梯度消失

在优化过程中，因为使用梯度下降的方法，在局部最小值和鞍点处梯度为零时，便不会继续优化下去。导致无法获得最优解。

当模型和损失函数形式较为简单时，误差最小化问题的解可以直接用公式表达出来。这类解叫作 **解析解**。然而大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作 **数值解**。

### 局部最小值

- 局部最小值（local minimum）：x邻近
- 全局最小值（global minimum）：整个定义域

![](image\7.1_output1.svg)

故当优化问题的数值解在局部最优解附近时，由于此时梯度近乎或变成0，最终迭代求得的数值解可能会令目标函数局部最小化而非全局最小化。

### 鞍点

梯度变成0可能会由两种情况造成：

1. 当前解在局部最优解附近
2. 当前解在鞍点（saddle point）附近

例：

- 对于给定函数 $f(x) = x^3$：

![](image\7.1_output2.svg)

- 对于二维空间函数 $f(x) = x^2-y^2$

![](image\7.1_output3.svg)

假设一个函数的输入为k维向量，输出为标量，那么它的海森矩阵有k个特征值。该函数在梯度为0的位置上可能是局部最小值、局部最大值或者鞍点。

**海森矩阵（Hession Matrix）**：由多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。$f(x) = x^2-y^2$的海森矩阵A为：

![](image\Hession_Matrix.jpg)

- 当函数的海森矩阵在梯度为零的位置上的特征值全为正时（正定矩阵），该函数得到局部最小值。
- 当函数的海森矩阵在梯度为零的位置上的特征值全为负时（负定矩阵），该函数得到局部最大值。
- 当函数的海森矩阵在梯度为零的位置上的特征值有正有负时（不定矩阵），该函数得到鞍点。例如上图二维空间函数，鞍点在x轴方向为局部最小值，y轴方向为局部最大值

另外，因为深度学习模型参数通常都是高维的（k很大），故目标函数的鞍点通常比局部最小值更常见。

### 梯度消失

![](image\q5p1jlrkib.svg)

在右上角开始进行梯度下降时，此时梯度很小，近乎为0，导致下降移动的非常缓慢。

## 凸性（凸函数）

### 集合

对集合内的任意两个点进行连线，连线上的点都在集合内的话，则该集合为一个 **凸集合**。

凸集合的交集仍是一个凸集合，两个凸集合的并集不一定是凸集合。

### 函数

满足如下条件，即为 **凸函数**：

$$\lambda{f(x)}+(1-\lambda){f(x')} \ge f(\lambda{x}+(1-\lambda){x'})$$

![](image\q5p1tqgzh5.svg)

左边和右边为凸函数，中间不为凸函数。

**注：** 此处凸函数定义与国际定义相同，与同济大学高数教材相反。

- 同济大学定义：二阶导数大于0，则为 **凹** 函数，有极小值；二阶导数小于0，则为 **凸** 函数，有极大值

![](image\20180918131716790.png)

- 国际上的定义：二阶导数大于0，则为 **凸** 函数，有极小值；二阶导数小于0，则为 **凹** 函数，有极大值

### Jensen不等式

结论：对于凸函数来说，函数值的期望大于期望的函数值。

### 凸函数性质

1. 无局部最小值
2. $f(x)$为凸函数，若$S_b:=\{x|x \in X and f(x) \le b\}$，则集合$S_b$为凸集。
3. 二阶条件

### 有限制条件的函数如何完成优化

1. 拉格朗日乘子法

$$L(x,\alpha) = f(x)+\sum_{i}{\alpha_ic_i(x) where \alpha_i \ge 0 }$$

2. 添加惩罚项
3. 进行投影
