# 优化
优化与深度学习的区别：
- 优化方法目标：降低训练集损失函数值
- 深度学习目标：降低测试集损失函数值（提高泛化性）

## 优化在深度学习中的挑战

1. 局部最小值
2. 鞍点
3. 梯度消失

在优化过程中，因为使用梯度下降的方法，在局部最小值和鞍点处梯度为零时，便不会继续优化下去。导致无法获得最优解。

当模型和损失函数形式较为简单时，误差最小化问题的解可以直接用公式表达出来。这类解叫作 **解析解**。然而大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作 **数值解**。

### 局部最小值

- 局部最小值（local minimum）：x邻近
- 全局最小值（global minimum）：整个定义域

![](image\7.1_output1.svg)

故当优化问题的数值解在局部最优解附近时，由于此时梯度近乎或变成0，最终迭代求得的数值解可能会令目标函数局部最小化而非全局最小化。

### 鞍点

梯度变成0可能会由两种情况造成：

1. 当前解在局部最优解附近
2. 当前解在鞍点（saddle point）附近

例：

- 对于给定函数 $f(x) = x^3$：

![](image\7.1_output2.svg)

- 对于二维空间函数 $f(x) = x^2-y^2$

![](image\7.1_output3.svg)

假设一个函数的输入为k维向量，输出为标量，那么它的海森矩阵有k个特征值。该函数在梯度为0的位置上可能是局部最小值、局部最大值或者鞍点。

**海森矩阵（Hession Matrix）**：由多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。$f(x) = x^2-y^2$的海森矩阵A为：

![](image\Hession_Matrix.jpg)

- 当函数的海森矩阵在梯度为零的位置上的特征值全为正时（正定矩阵），该函数得到局部最小值。
- 当函数的海森矩阵在梯度为零的位置上的特征值全为负时（负定矩阵），该函数得到局部最大值。
- 当函数的海森矩阵在梯度为零的位置上的特征值有正有负时（不定矩阵），该函数得到鞍点。例如上图二维空间函数，鞍点在x轴方向为局部最小值，y轴方向为局部最大值

另外，因为深度学习模型参数通常都是高维的（k很大），故目标函数的鞍点通常比局部最小值更常见。

### 梯度消失

![](image\q5p1jlrkib.svg)

在右上角开始进行梯度下降时，此时梯度很小，近乎为0，导致下降移动的非常缓慢。

## 凸性（凸函数）

### 集合

对集合内的任意两个点进行连线，连线上的点都在集合内的话，则该集合为一个 **凸集合**。

凸集合的交集仍是一个凸集合，两个凸集合的并集不一定是凸集合。

### 函数

满足如下条件，即为 **凸函数**：

$$\lambda{f(x)}+(1-\lambda){f(x')} \ge f(\lambda{x}+(1-\lambda){x'})$$

![](image\q5p1tqgzh5.svg)

左边和右边为凸函数，中间不为凸函数。

**注：** 此处凸函数定义与国际定义相同，与同济大学高数教材相反。

- 同济大学定义：二阶导数大于0，则为 **凹** 函数，有极小值；二阶导数小于0，则为 **凸** 函数，有极大值

![](image\20180918131716790.png)

- 国际上的定义：二阶导数大于0，则为 **凸** 函数，有极小值；二阶导数小于0，则为 **凹** 函数，有极大值

### Jensen不等式

结论：对于凸函数来说，函数值的期望大于期望的函数值。

### 凸函数性质

1. 无局部最小值
2. $f(x)$为凸函数，若$S_b:=\{x|x \in X and f(x) \le b\}$，则集合$S_b$为凸集。
3. 二阶条件

### 有限制条件的函数如何完成优化

1. 拉格朗日乘子法

$$L(x,\alpha) = f(x)+\sum_{i}{\alpha_ic_i(x) where \alpha_i \ge 0 }$$

2. 添加惩罚项
3. 进行投影

# 梯度下降

## 一维梯度下降

沿梯度反方向移动自变量可以减小函数值

$$f(x-\eta{f'(x)}) = f(x)-\eta{f'^2(x)}+O({\eta}^2f'^2(x))$$

更新方式：

$$x \leftarrow x-\eta{f'(x)}$$

### 学习率

学习率过小会导致梯度下降的很慢

![](image\q5oim918r4.svg)

学习率过大可能会导致梯度呈一个发散的情况，收敛不到最小值

![](image\q5oim98mjm.svg)

## 多维梯度下降

$$\nabla{f(x)} = \Bigg[\frac{\partial{f(x)}}{\partial{x_1}},...,\frac{\partial{f(x)}}{\partial{x_d}}\Bigg]^T$$

更新方式：

$$x \leftarrow x-\eta{\nabla{f(x)}}$$

以上所有公式中$\eta$均指 **学习率**

例：

$$f(x)={x_1}^2+2{x_2}^2$$

梯度下降：沿垂直等高线方向下降

![](image\q5oim9cca5.svg)

## 自适应方法（计算复杂性较高，应用较少）

### 牛顿法

对于$f(x+\epsilon)$，在$x+\epsilon$处泰勒展开（到二阶导数）后关于$\epsilon$求导并忽略高阶无穷小，有：

$$\nabla{f(x)}+H_f{\epsilon}=0 \; and\;hence\;\epsilon=-H_f^{-1}\nabla{f(x)}$$

即$x$移动$\epsilon$的长度，$H_f$为海森矩阵的值，$\nabla{f(x)}$为$f(x)$的梯度

但牛顿法对于有局部最小值的函数，仍可能无法正确进行梯度下降，需要调整学习率才可。

### 预处理（Hessian矩阵辅助梯度下降）

不直接使用学习率$\eta$乘以x的梯度($\nabla{x}$)，而是先除以海森矩阵对角线上的值，再乘以x的梯度($\nabla{x}$)

$$x \leftarrow x-\eta{diag(H_f)^{-1}}\nabla{x}$$

即基于牛顿法的高维梯度更新公式，保证了不同维度上的收敛。

### 共轭梯度法

## 随机梯度下降
### 参数更新

对于有n个样本的训练数据集，设$f_i(x)$是第i个样本的损失函数，则目标函数是：

$$f(x) = \frac{1}{n}\sum^{n}_{i=1}f_i(x)$$

其梯度为：

$$\nabla{f(x)}=\frac{1}{n}\sum^{n}_{i=1}\nabla{f_i(x)}$$

使用该梯度的一次更新的时间复杂度为$O(n)$

而使用随机梯度下降更新为：

$$x \leftarrow x-\eta{\nabla{f_i(x)}}$$

随机梯度下降的时间复杂度为$O(1)$

例：对于$f(x1,x2)=x_1^2+2x_2^2$:

![](image\q5oimaluhh.svg)

可发现在右上角最优点处一直抖动，通过动态学习率解决

### 动态学习率
共分三种：

1. 分段常数函数：

$$n(t)=\eta_i\;if\;t_i\le{t}\le{t_{i+1}}$$

2. 指数函数

$$n(t)=\eta_0 \cdot e^{-\lambda{t}}$$

3. 多项式函数

$$n(t)=\eta_0 \cdot (\beta{t}+1)^{-\alpha}$$

指数函数修改学习率：

![](image\q5oimas9qm.svg)

可见通过该方法可有效修改缓解最优点处的抖动情况

## 小批量随机梯度下降

- 梯度下降：每次数据更新使用整个数据集上所有样本的平均梯度进行更新。对于数据集中相似数据较多的情况，数据的利用率较低。
- 随机梯度下降：每次对参数进行更新时，使用单个样本的梯度进行更新，未充分利用GPU的矢量运算能力
- 小批量梯度下降：上述二者的折中，选择部分样本梯度的均值对参数进行更新。