# 批量归一化

可以使深层网络的训练变得更加容易，本质是对数据的标准化处理。即在模型训练时，利用小批量上的均值与标准差，不断调整神经网络的中间输出，从而使得神经网络在各层的中间输出的数值变得更加稳定。

## 对全连接层做批量归一化


输入为$u$，大小为$batch\_ size \times 输入神经元个数$

$$x = Wu + b$$
$x$的大小为 $batch\_size \times 输出神经元个数$

位置：对$x$先做批量归一化(BN)，再把结果通过激活函数输出

$$y^{(i)} = BN(x^{(i)})$$

$$期望：\mu_B \leftarrow \frac{1}{m} \sum^{m}_{i=1}x^{(i)}$$

$$方差：\sigma^{2}_{B} \leftarrow \frac{1}{m} \sum^{m}_{i=1}(x^{(i)}-\mu_{B})^2$$

上式中$m$为$batch\_size$的大小

$$标准化后结果：\hat{x}^{(i)} \leftarrow \frac{x^{(i)}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+ \epsilon}}$$

其中 $\epsilon > 0$是个很小的常数，用来保证分母大于0

$$y^{(i)} \leftarrow \gamma \odot \hat{x}^{(i)}+ \beta$$

拉伸参数$\gamma$和偏移参数$\beta$为可学习参数，可保留批量归一化无效的可能性。

当批量归一化的效果不好时，可通过使$\gamma = \sqrt{\sigma_{B}^{2}+ \epsilon}$，$\beta = \mu_{B}$，使得 $\hat{x}^{(i)} = x^{(i)}$，来实现批量归一化的无效化

## 对卷积层做批量归一化

位置：卷积计算之后，应用激活函数之前

二者区别：
 - 全连接层（2维）：$（batch\_size大小）m \times d（输出神经元个数）$
 - 卷积层（4维）：$m（样本数） \times c（通道数） \times p（卷积后的高） \times q（卷积后的宽）$
 - 对于全连接，是对$m$个元素做批量归一化；对于卷积层，对$m \times p \times q$个元素同时做批量归一化，使用相同的均值和方差。
 - 如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数。

 ## 预测时做批量归一化
 - 训练：以batch为单位,对每个batch计算均值和方差
 - 预测：通过平均估算整个训练数据集的样本均值和方差来对预测的数据做标准化，估算的方法为 **移动平均法**

 ```
 # 更新移动平均的均值和方差
moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
moving_var = momentum * moving_var + (1.0 - momentum) * var
 ```

`momentum` 是一个定义的超参数，不通过学习获得

直接调用时：
- BatchNorm2d：卷积层的批量归一
- BatchNorm1d：全连接层的批量归一

# 残差网络

深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，准确率也变得更差。

## 残差块
恒等映射：
- 左边（普通神经网络层）：$f(x) = x$
- 右边（残差块）：$f(x)-x=0$（易于捕捉恒等映射的细微波动）

![](image\q5l8lhnot4.png)

当残差块中，$f(x)-x$ 的通道数与 $x$ 的通道数不同时，需要借助 $1 \times 1$ 卷积层来改变通道数。

# 稠密连接网络
是残差网络的衍生设计

![](image\q5l8mi78yz.png)

主要构建模块：
- 稠密块：定义了输入和输出是如何连接的
- 过渡层：用来控制通道数，使之不过大

## 稠密块

```
class DenseBlock(nn.Module):
    def __init__(self, num_convs, in_channels, out_channels):
        super(DenseBlock, self).__init__()
        net = []
        for i in range(num_convs):
            in_c = in_channels + i * out_channels
            net.append(conv_block(in_c, out_channels))
        self.net = nn.ModuleList(net)
        self.out_channels = in_channels + num_convs * out_channels # 计算输出通道数
```

- num_convs：调用的卷积层数
- in_channels：整个DenseBlock的输入通道数
- out_channels：不代表整个DenseBlock的最后输出通道数，而是代表每做一次卷积所输出的out_channels

在上图DenseNet中，A的输入为in_channels，B的输出为out_channels，最后拼接结果为in+out，作为下一个A的输入，n个卷积层的话，输出为 `in+n*out`

因此有：

```
out_channels = in_channels + num_convs * out_channels
```

## 过渡层

- $1 \times 1$卷积层：减小通道数
- 步幅为2的平均池化层：减半高和宽
