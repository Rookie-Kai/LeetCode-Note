### 梯度消失与爆炸
消失与爆炸是深度模型有关数值稳定性的问题

多层感知机第$l$层的$H^{(l)}$的权重参数是$W^{(l)}$，不考虑偏差参数的情况下，输出为

$$H^{(l)} = XW^{(1)}W^{(2)}...W^{(1)}$$
故当 **神经网络层数较多** 时，模型的 **数值稳定性** 容易变差， **梯度的计算** 也容易消失或爆炸。

**注：在深层网络中尽量避免选择 sigmoid 和 tanh 激活函数，因为二者会把元素转换到[0,1]和[-1,1]之间，加剧梯度消失。**

---
### 随机初始化模型参数
神经网络模型在层间各个单元间具有对称性，若每个隐藏单元的参数都初始化为相同的值，在正向传播时每个隐藏单元将根据相同的输入计算出相同值传至输出层。在反向传播时，每个隐藏单元的参数梯度值相等，通过基于梯度的优化算法迭代无论多少次后仍相等，相当于仅有一个隐藏单元在发挥作用。所以常将神经网络的模型参数，特别是权重参数进行随机初始化。

#### PyTorch的默认随机初始化
在PyTorch中，`nn.Module`的模块参数均采用合理的初始化策略，具体见其源码，一般不需要我们考虑

#### Xavier随机初始化
在Xavier随机初始化方法中，权重参数的每个元素都随机采样于均匀分布

$$U(-\sqrt{\frac{6}{a+b}},\sqrt{\frac{6}{a+b}})$$
$a$为全连接层输入个数，$b$为全连接层输出个数。

在模型参数初始化后，每层输出的方差不该受该层输入个数影响，每层梯度的方差也不该受该层输出个数影响

---
### 环境因素对模型训练与应用的影响
#### 协变量偏移
输入的分布$P(x)$可能随时间而改变，但标记函数$P(y|x)$不会改变

![](image/协变量.jpg)

#### 标签偏移
标签$P(y)$的边缘分布发生变化，但其类条件分布$P(x|y)$不变

例如：**病因**（需预测的诊断结果）导致 **症状**（观察到的结果）

训练数据集的数据很少，只包含流感$P(y)$的样本

测试数据集有流感$P(y)$和流感$Q(y)$，其中不变的是流感症状$P(x|y)$，此时就认为发生了 **标签偏移**。

![](image/标签偏移.jpg)
#### 概念偏移
相同的概念由于地理位置不同，其定义也发生变化。