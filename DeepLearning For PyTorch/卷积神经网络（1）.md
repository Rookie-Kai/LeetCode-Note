# 二维卷积层
- 卷积神经网络：含有卷积层的神经网络
- 二维卷积层：含有高和宽两个空间维度，常用来处理图像数据
## 二维互相关运算
在二维卷积层中，二维输入数组和二维核数组通过互相关运算输出一个二维数组。

核数组在卷积计算中又称卷积核或过滤器（fliter)

![](image\5.1_correlation.svg)

在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。

滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。

## 二维卷积层
二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。

卷积层的模型参数包括了卷积核和标量偏差。

在训练模型的时候，通常先对卷积核随机初始化，然后不断迭代卷积核和偏差。

卷积窗口为$p \times q$的卷积层称为$p \times q$卷积层

卷积层可通过重复使用卷积核有效表征局部空间

卷积核可通过数据来学习

## 互相关运算与卷积运算
卷积运算类似互相关运算，将核数组左右翻转并上下翻转，再与输入数组做互相关运算，即可得到 **卷积运算的输出** 。

故对于同一个输入数组，若卷积运算与互相关运算采用相同的核数组，二者输出往往不相同

因为在深度学习中，核数组都是学习获得的，卷积层无论使用互相关运算亦或卷积运算均不影响模型预测时的输出。

## 特征图与感受野
- 特征图：二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征
- 感受野：影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野

# 填充与步幅
对于一个二维卷积层：

假设输入形状为$n_h \times n_w$，卷积核窗口形状是$k_h \times k_w$，则其输出形状为

$$(n_h-k_h+1) \times (n_w-k_w+1)$$
故卷积层的输出形状由输入形状和卷积核窗口形状决定。

填充与步幅是卷积层的两个超参数，可以对给定形状的输入和卷积核改变输出形状。

## 填充
在输入高和宽的两侧填充元素（通常是0元素）。

![](image\5.2_conv_pad.svg)

上图在原输入高和宽的两侧分别添加了值为0的元素，使得输入高和宽从3变成了5，并导致输出高和宽由2增加到4。

一般来说，如果在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，输出的高和宽会分别增加$p_h$和$p_w$，那么输出形状将会是：

$$(n_h-k_h+p_h+1) \times (n_w-k_w+p_w+1)$$

常设置$p_h=k_h-1$，$p_w=k_w-1$使得输入与输出具有相应的高和宽。便于在构造网络时推测每个层的输出形状。

填充可以增加输出的高和宽

## 步幅
在二维互相关运算中，卷积窗口从输入数组的最左上方开始，从左至右从上至下，依次在输入数组上滑动。每次滑动的行数和列数被称为 **步幅** 。

![](image\5.2_conv_stride.svg)

上图高和宽上步幅分别为3和2的二维互相关运算

当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：

$$\lfloor(n_h-k_h+p_h+s_h)/s_h \rfloor \times \lfloor (n_w-k_w+p_w+s_w)/s_w \rfloor$$

步幅可用来减小输出的高和宽

在进行卷积运算和池化的时候，对于输入图像大小为input_size，给定kernel_size、padding、stride，计算得出output_size为：

```
output_size =1+ (input_size+2*padding-kernel_size)/stride
```


# 多输入通道和多输出通道

彩色图像在高和宽两个维度外，还有RGB三个颜色通道，假设其高和宽分别为$h$和$w$，则可表示为一个$3 \times h \times w$的多维数组，大小为3的这一维被称为 **通道维** 。

多通道可拓展卷积层的模型参数。

## 多输入通道

当输入数据含有多个通道时，需要构造一个**输入通道数** 与 **输入数据的通道数** 相同的卷积核，从而与含有多通道的输入数据完成互相关运算。

![](image\5.3_conv_multi_in.svg)

阴影部分的计算为：

$$(1 \times 1+2 \times 2+4 \times 3+5 \times 4)+(0 \times 0+1 \times 1+3 \times 2+4 \times 3) = 56$$

## 多输出通道

当含有多个输入通道时，因为我们对各个通道的结果进行了累加，所以不论输入通道数是多少，输出通道数总为1。

设卷积核输入通道数和输出通道数为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。若想获得含有多个通道的输出，可以为每个输出通道创建形状为$c_i \times k_h \times k_w$的核数组，将其在输出通道上连结，卷积核的形状即$c_o \times c_i \times k_h \times k_w$

在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。

对于输出通道的卷积核，我们提供这样一种理解，一个$c_i \times k_h \times k_w$的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的$c_i \times k_h \times k_w$的核数组，不同的核数组提取的是不同的特征。

## $1 \times 1$ 卷积层

卷积窗口形状为$1 \times 1(k_h=k_w=1)$的多通道卷积层常被称为$1 \times 1$卷积层。

因为使用了最小窗口，$1 \times 1$卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。$1 \times 1$卷积的计算主要发生在通道维上。

![](image\5.3_conv_1x1.svg)

输入与输出具有相同的高和宽，输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。

若将通道维当作特征维，将高和宽维度上的元素当成数据样本，则 **$1 \times 1$卷积层的作用与全连接层等价**

故$1 \times 1$卷积层可被当作保持高和宽维度形状不变的全连接层使用。常用来调整网络层之间的通道数控制模型复杂度。

