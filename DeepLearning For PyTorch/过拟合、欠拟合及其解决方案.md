## 模型选择、过拟合与欠拟合
### 训练误差与泛化误差
  - 训练误差：训练的模型在训练集上表现的误差
  - 泛化误差：模型在泛化样本（训练集以外的全部样本）上表现的误差，常用测试集上表现的误差来近似

可以用损失函数来计算训练误差与泛化误差，机器学习模型训练好后常应用于不同泛化场景，所以模型的关注点在于降低泛化误差。

---
### 模型选择
#### 验证数据集
测试集在超参数与模型参数选定后只能使用一次，故不可使用测试集选择模型。又因为无法通过训练误差估计泛化误差，所以不可只依赖训练数据选择模型。因此需要在给定的训练集中随机选取一小部分作为验证集，剩余部分作为真正的测试集。
 - 超参数：在开始学习之前设置的参数，而不是通过训练得到的参数数据。需要对超参数进行优化以后，提供给模型一组最优超参数，来提高学习的效果。即人为通过经验设置的值。
 - 超参数示例：
    - 学习率
    - 深层神经网络的隐藏层数
    - K-means聚类中的簇数
#### K折交叉验证
常用于训练数据不足的情况，将原始训练数据集分割为K个不重合的子集，使用一个子集作为验证集，其他K-1个子集作为训练集。每次使用不同子集作为验证集，训练验证K次。

---
### 过拟合与欠拟合的影响因素
 - 欠拟合：模型无法得到较低的训练误差
 - 过拟合：模型的训练误差低但是泛化误差高，使得训练误差远小于泛化误差
#### 模型复杂度
模型复杂度与误差的关系如下：

![](image/Model_complexity.png)

#### 训练集大小
如果训练集过少，很容易发生过拟合现象，所以在条件允许的范围内，应该尽量增大训练集。

---
### 权重衰减
权重衰减等价于$L_2$范式正则化。
为应对过拟合，常通过正则化对模型的损失函数添加惩罚项，使得学出的模型参数值较小。权重衰减便通过惩罚绝对值较大的模型参数为模型增加限制，有效应对了过拟合。

#### $L_2$范数正则化
即在原损失函数基础上添加了$L_2$范数惩罚项，只影响权重参数，不改变梯度。
- $L_2$范数惩罚项：模型权重参数每个元素的平方和与一个正的常数的乘积。
- `fit_and_plot(lambd)` 函数
  - lambd是一个人为定义的超参数，也就是$L_2$范数惩罚项中的正的常数。

- 通过PyTorch实现的函数 `fit_and_plot_pytorch(weight_decay)`
  - 参数的初始化通过 `init` 模块完成
  - 优化函数通过`optim` 模块里的`SGD`（随机梯度下降）函数完成，权重衰减已封装在`SGD`函数中，只需要通过对`weight_decay`参数输入值来启用或关闭权重衰减。

---
### 通过丢弃法防止过拟合
以丢弃概率作为丢弃法的超参数，对神经网络的隐藏单元使用。设丢弃概率为$p$，则隐藏单元会有$p$的概率被清零，有$1-p$的概率会除以$1-p$做拉伸。
- 设随机变量$\xi_i$为0和1的概率分别为$p$和$1-p$，为0时清零，为1时拉伸。通过丢弃法计算的新隐藏单元为

$${h_i}^{'} = \frac{\xi_i}{1-p}h_i$$
$\xi_i$的期望$E(\xi_i) = 1-p$，故

$$E({h_i}^{'}) = \frac{E(\xi_i)}{1-p}h_i = h_i$$
所以丢弃法不改变输入的期望值。

对神经网络的多个隐藏层可以设置多个丢弃率，神经网络的隐藏层在使用丢弃法清零隐藏单元后，在反向传播过程中与对应隐藏单元相关的权重梯度均为0，使得该隐藏单元在输出层的计算中不被依赖，从而起到正则化的作用，应对过拟合。

但在测试模型时，为获得更加确定的结果，一般不使用丢弃法。

#### 丢弃法的从零实现 - `dropout`函数
 - `drop_prob`为丢弃率
 - `assert`检验丢弃率是否处在 (0,1) 的范围内，否则Error
 - `keep_prob`为保存率,为0时丢弃全部元素

 #### 丢弃法的PyTorch实现
  - 调用`nn.Dropout(drop_prob)`