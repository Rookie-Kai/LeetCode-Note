### 权重共享

对于同一深度的神经元是共享参数的，因此也导致其对于平移是不敏感的

---

### 局部连接

每一个神经元仅与输入神经元的一个块域相连接

---

### AlexNet的创新

- 采用全新激活函数 ReLU函数

$$ReLU(x) = max(0,x)$$

- Dropout机制
    - 休眠部分神经元
    - 起到正则化作用，减轻过拟合

---

### ZFNet

- AlexNet基础上进行细节改动
- 主要贡献
    - 采用 **反卷积** 进行卷积神经网络的可视化

---

### GoogleNet

- 创新点
    - 采用 **Inception机制** 解决过拟合和参数过多的问题

#### Inception机制
- 核心思想
    - 多尺度处理
    - 减少模型的参数数量
- 做法
    - 整合不同尺度的卷积核、池化层
- 结果
    - 参数量减少到AlexNet的 1/12

---

### 对GoogleNet的Inception进行改进
- Inception-V2（GoogleNet-BN）
    - 加入了BN层
        - 使每一层的输出都规范到0-1的高斯分布上
    - 学习了VGG，用3x3的conv代替了Inception模块中的5x5
        - 降低了参数数量，加快了计算速度

- Inception-V3
    - 卷积核分解
    - 提出了RMSProp （新网络训练优化策略）
    - Label Smoothing

---

### VGG网络
- 优势
    - 拓展性、泛化性好
    - 结构简洁
- 相比AlexNet的改进
    - 去掉了LRN层（局部响应归一化层）
    - 采取3x3卷积核模拟大尺寸卷积核
        - 可用两个3x3实现一个5x5卷积核的局部感受野。两个3x3共18个参数，一个5x5共25个参数，降低了参数数量

--- 

### 残差网络（ResNet)

- 跨层连接拟合残差项
    - 网络中的数据可以选择性跳过一些网络层，与该网络层的下一网络层直接连接进行运算

- 残差网络的优势
    - 通过shortcut解决了模型退化的问题，实现了模型加深，准确率不下降。

---

### ResNeXt
- 主要目的：在不增加参数数量的前提下提高准确率
- 网络结构
    - VGG堆叠+Inception的split-transform-merge思想

---

### DenseNet
- 是一种具有密集连接的卷积神经网络
    - 任何两层之间都有直接的连接，每一层的输入都是前面所有层输出的并集，该层学习的feature Map（特征图）也会传给后面所有层作为输入
- 网络优点
    - 网络更窄、参数更少
    - 减轻过拟合
    - 对小数据集合的学习很有效果
- 缺点
    - 显存占用没有明显优势
    - 计算量没有明显减少

---

### MobileNet

- 针对手机等嵌入式设备提出的轻量级深度神经网络
- 通过 **分离卷积** 可以在牺牲较小性能的前提下有效减小参数量和计算量
- 分类卷积
    - 将传统卷积运算用两步卷积运算代替
        - Depthwise convolution
        - Pointwise convolution
    - Depthwise convolution
        - filter的数量只能为3，传统卷积方法会有3x3共9个filter
    - MobileNet-v2增加了残差结构，在Depthwise convolution之前添加了一层Pointwise convolution，优化了带宽的使用，进一步优化了在嵌入式设备上的性能

---

### WRN（wide residual network）
- 设计思路
    - 原因：梯度无法保证流经每一个模块
    - 结果：导致模块使用率较低
    - 解决方案：设计层数较前，但是宽度更宽的模型
- 实验结果
    - WRN通过增加channel的数量来加宽模型
    - 单独增加宽度提升了模型性能

--- 

### NASNet
- 作用
    - 在小数据集上自动设计出卷积神经网络
    - 通过迁移学习，可以迁移数据集或计算机视觉任务
- 方法
    - 强化学习
    - ResNet和Inception等成熟的网络拓扑结构