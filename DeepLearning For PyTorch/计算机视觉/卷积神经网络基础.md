## **时延卷积神经网络**
- 对于语音信号可以很好的分类
- 对于图像信息，若直接输入一张图片，会导致巨大计算负担
- 隐含层结点个数等于滤波器个数

全连接方法会对图像上每一个像素分配一个权重，200*200的图像会造成40000个权重生成

### **解决思路---（降低权重数量）**
- 视觉系统局部感受野：人的视觉神经元只接收局部信息
- 图像空间相关性：距离近的相关性大，距离远的相关性小。

#### 一、由全连接变为局部连接
- 每个神经元只对局部信息进行感知
- 在更高层将局部信息进行综合，获得全局信息

#### 二、权值共享
- 局部连接可降低网络权重数量，但还不够
- 权值共享机制：从图像的任一局部区域连接到同一类型的隐含结点，其权值不变



例：若200 * 200像素的图像，共4000个滤波器（卷积核）

- 全连接：一个滤波器有40000个权重，总共40000 * 4000，共1亿6千万权重
- 采用5 * 5大小的窗口，每个隐含节点有25个权重，4000*25，共10w权重

图像滤波操作：即 **互相关操作** ，image * fliter，输入数组*卷积核

![](image\5.1_correlation.svg)

#### 局部连接+权值共享

对于200 * 200的图片，经过和卷积核（5 * 5）互相关运算后（滤波处理）的大小为

$$(200-5+1) \times (200-5+1) = 196 \times 196$$

- 若采用全连接方式，输出图像的每一个像素点都会与原图的像素点进行连接，权重数为：$200 \times 200 \times 196 \times 196$
- 若仅采用局部连接，输出图像的一个像素点会连接原图的一个局部区域，权重数为：$25 \times 196 \times 196$
- 若采用局部连接+权值共享的方法，$196 \times 196$个滤波器共享25个权值，最终权重数量为：25

一个滤波器可学习一种特征，在实际应用中，常使用多个滤波器学习多种特征。

例：若含有四个滤波器
- 采用全连接方案：$200 \times 200 \times 196 \times 196 \times 4$
- 局部连接+权值共享的方法：$25 \times 4$

### **卷积神经网络的设计**

由上述可知，在多个滤波器的场景下，采用全连接方法，会因权值过多产生高维问题，使用高维数据设计分类器，会导致过拟合。

对于上述问题的解决方案是：**对不同位置特征进行聚合统计，例如计算图像上某个区域上某特定特征的平均值或最大值（池化层）**，从而有效减少过拟合问题。

下述中：滤波器即为卷积核，滤波操作即为互相关运算

#### 第一个隐含层的设计

输入一个 200 * 200的图片，输入4个的图片

1. 对图像采用多个滤波器进行滤波操作，获得多个滤波计算结果，每一个结果对应一种特征
2. 对每一个局部滤波结果做一个非线性激活
3. 对每个滤波结果图像做pooling（池化）操作，图像大小得到降低（步幅大小和池化窗口形状相同）。即通过pooling操作，完成了图像下采样（downsample）
4. 第一层到第二层：输入1个200 * 200的图片 $\rightarrow$ 经过5 * 5的滤波器$\rightarrow$ 尺寸变为196 * 196 $\rightarrow$ 经过2 * 2的池化层 $\rightarrow$ 输出4个尺寸变为 98 * 98 输入第二个隐含层

#### 第二个隐含层的设计

输入4个98 * 98的图片，输出7个图像，采用3 * 3的滤波器

输入4个图像，输出7个图像，共有28个不同滤波器需要学习

每个滤波器有 3 * 3个参数，共有 28 * 3 * 3个参数

输出为：

$$Y_j^L = f\Big(\sum_i(y_i^{L-1} \times K_{ij})+w_j\Big)$$

f为激活函数，$w_j$为阈值

在第一隐含层到第二隐含层中：由于第一隐含层包含4个图像，即4个大的结点集群（图像），需要将这些大的结点集群的输出值进行卷积累加，累加到第二隐含层的同一结点集群。**这就是前向神经网络的普遍操作特点。**

**如果将4个卷积结果排列成一个立体，这种卷积（累加）操作也可在立体数据上进行**

例：将上述例子中的4个输出图像看成一个立方体，可称之为volume，待学习的滤波器为一个 **三维** 滤波器。三维滤波器对立体数据进行卷积操作。**此时只需要学习7个三维滤波器**

#### 第一隐含层到第二隐含层的权重数

- 采用全连接：输入 * 输出 = （4 * 98 * 98）* （4 * 98 * 98）
- 采用局部连接 + 权值共享（3 * 3 滤波器）：9 * 4 * 7

#### 层于层之间的基本操作

- 卷积
    - 将卷积之和加到下一层
    - 对卷积之和进行激励（也可在Pooling层之后求激励）
- 池化（Pooling）
    - 下采样，两种基本运算：窗口取均值或窗口取最大值

### **卷积神经网络的训练方式**

#### 网络训练

- 采用反向传播算法
    1. 选择一个样本x，信息从输入层经过逐级变换，传送到输出层
    2. 计算该样本的实际输出o与相应的理想输出t的差
    3. 按极小化误差反向传播方法调整权重矩阵
    4. 可以采取小的样本组，逐步进行训练

#### 核心思想

- 之所以采用卷积后的特征，是因为图像具有 **静态性**，即在一个图像区域有效的特征，可能可以在另一个区域同样使用
- 局部感受野、权值共享（权值复制）以及时间或空间下采样这三种结构化思想结合起来获得了某种程度的 **位移、尺度、形变** 的不变性。