# 深度卷积神经网络（AlexNet）

神经网络可以基于图像的原始像素进行分类，这种方法被称为端到端（end to end）的方法，可以节省很多步骤。

在计算机视觉流程中，真正重要的是 **数据** 和 **特征**，即使用较干净的数据集和较有效的特征比选择机器学习模型对图像的影响更大。

## 学习特征表示

在很长时间内，通过神经网络学习视觉数据的逐级表征未能实现，原因如下：

 - **缺失数据：** 包含许多特征的深度模型需要大量的有标签的数据才能表现得比其他经典方法更好，这一状况在2010年前后兴起的大数据浪潮中得到改善

 - **缺失硬件：** 深度学习对计算资源要求很高。早期的硬件计算能力有限，这使训练较复杂的神经网络变得很困难。直到通用GPU的到来改变了这一格局

## AlexNet

首次证明了学习到的特征可以超越⼿⼯设计的特征，从而⼀举打破计算机视觉研究的前状。

AlexNet的网络结构如下：

![](image\5.6_alexnet.png)

**特征：**

- 8层变换，包括5层卷积层，2层全连接隐藏层，1个全连接输出层
- 把$sigmoid$函数改为更简单的$ReLU$激活函数
- 用 **Dropout（丢弃法）** 来控制全连接层的模型复杂度
- 引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合

**对比LeNet：**

![](image\FPwlSxNxy1uT8oYYdAqrA.png!png)

AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线

# 使用重复元素的网络（VGG）

VGG提出了可通过重复使用简单的基础块来构建深度模型的思路

## VGG块

组成规律：连续使用数个相同的填充为1、窗口形状为$3 \times 3$的卷积层后接上一个步幅为2、窗口形状为$2 \times 2$的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。

> **对于给定的感受野，采用堆积的小卷积核优于采用大的卷积核，因为这样可以增加网络深度来保证学习更复杂的模式，而且代价较小（参数少）**

## VGG网络

与AlexNet和LeNet相同，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块的超参数由`conv_arch`定义，该变量指定了每个VGG模块里的卷积层个数和输入输出通道数。全连接模块则和AlexNet中的相同。

VGG Block：数个相同的填充为1、窗口形状为$3 \times 3$的卷积层,接上一个步幅为2、窗口形状为$2 \times 2$的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。

![](image\by2wiwCbEl8N4NbZGQh-9.png!png)

# 网络中的网络（NiN）

对比：
- **LeNet、AlexNet和VGG：** 先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。AlexNet和VGG对LeNet的改进主要在如何对这两个模块进行加宽（增加通道数）和加深。
- **NiN：** 串联多个由卷积层和“全连接层”构成的小网络来构建一个深层网络。

## NiN块
NiN中的基础块，由一个卷积层加两个充当全连接层的$1 \times 1$卷积层串联而成，第一个卷积层的超参数可自行设置，第二三个$1 \times 1$卷积层的超参数一般是固定的。

NiN使用$1 \times 1$卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去，对比如下：

![](image\5.8_nin.svg)

$1 \times 1$卷积核的作用：
- 放缩通道数：通过控制卷积核的数量达到通道数的缩放
- 增加非线性：$1 \times 1$卷积核的卷积过程相当于全连接层的计算过程，并且加入了非线性的激活函数，从而增加网络的非线性
- 计算参数少

![](image\EnnR73QZiJHhMLQlLrNgL.png!png)

NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块，然后使用全局平均池化层（窗口形状等于输入空间维形状）对每个通道中所有元素求平均并直接用于分类。

通过上述设计，NiN可显著减小模型的参数尺寸，缓解过拟合，但有时会增加获取有效模型的训练时间。

# 含并行连结的网络（GoogleNet）

## Inception块
GoogleNet的基本卷积块，共四条并行线路，结构如下：

![](image\5.9_inception.svg)

- 前三条线路：使用$1 \times 1，3 \times 3，5 \times 5$的卷积层抽取不同空间尺寸下的信息。
  - 第一条线路：使用$1 \times 1$卷积层来减少通道数
  - 中间两条线路：对输入先做$1 \times 1$卷积来减少通道数，降低模型复杂度

- 第四条线路：使用$3 \times 3$最大池化层，后接$1 \times 1$卷积层来改变通道数。

Inception块相当于一个有4条线路的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用$1 \times 1$卷积层减少通道数从而降低模型复杂度。

Inception块中可以自定义的超参数是每个层的输出通道数，以此来控制模型复杂度

## GoogleNet模型
跟VGG一样，在主体卷积部分中使用5个模块（block），每个模块之间使用步幅为2的3×33×3最大池化层来减小输出高宽。

- 第一模块使用一个64通道的$7 \times 7$卷积层。
- 第二模块使用2个卷积层：
    - 64通道的$1 \times 1$卷积层
    - 将通道增大3倍的$3 \times 3$卷积层，对应Inception块中的第二条线路。
- 第三个模块串联2个完整的Inception块：
    - 第一个Inception块的输出通道数为 64 + 128 + 32 + 32 = 256，其中4条线路的输出通道数比例为 64 : 128 : 32 : 32 = 2 : 4 : 1 : 1。
        - 其中第二、第三条线路先分别将输入通道数减小至 96/192 = 1/2 和 16/192 = 1/12 后，再接上第二层卷积层。
    - 第二个Inception块输出通道数增至 128 + 192 + 96 + 64 = 480，每条线路的输出通道数之比为 128 : 192 : 96 : 64 = 4 : 6 : 3 : 2。
        - 其中第二、第三条线路先分别将输入通道数减小至 128 / 256 = 1 /2 和 32 / 256 = 1 / 8。
- 第四个模块串联了5个Inception块：
    - 其输出通道数分别是：
        - 192 + 208 + 48 + 64 = 512
        - 160 + 224 + 64 + 64 = 512
        - 128 + 256 + 64 + 64 = 512
        - 112 + 288 + 64 + 64 = 528
        - 256 + 320 + 128 + 128 = 832
- 第五个模块有输出通道数为 256 + 320 + 128 + 128 = 832 和 384 + 384 + 128 + 128 = 1024 的两个Inception块。
- 输出层紧接在第五个模块后，与NiN一样使用全局平均池化层（Global AvgPool）将每个通道的高和宽变成1，将输出变成二维数组后接上一个输出个数为标签类别数的全连接层（Dense）。

![](image\MUadJ3t0BoML0xJ-IR-SZ.png!png)