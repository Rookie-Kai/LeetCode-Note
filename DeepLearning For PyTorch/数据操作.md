## 数据操作
### 1.view( )函数
- 用 **`view()`** 来改变 **`Tensor`** 的形状：

  - <code>**view(3,2)**</code>：转化为3行2列
 
  - <code>**view(3,-1)**</code>：-1所指的维度可以根据其他维度推算出来，例如此处共6个元素，此时-1就代表2.
 
---
 
### 2.np.random.normal(loc, scale, size)
 - 生成正态分布的概率密度随机数
   - **loc：** 此概率分布的均值
   - **scale：** 此概率分布的标准差，越大越矮胖，越小越瘦高
   - **size：** 输出的 **`shape`** ，默认为None,只输出一个值
```
nd1 = np.random.normal(loc=1,scale=2,size=2)
#array([-0.46982446, -1.28956852])
```

---

### 3.init.normal_(tensor, mean, std)
- 通过Pytorch提供的**`init`**模块，实现正态分布
  - **tensor：** 输入的张量
  - **mean：** 均值
  - **std：** 标准差
```
init.normal_(net[0].weight, mean=0, std=0.01)
# 将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。
```

---

### 4.练习

在线性回归模型中，对于某个大小为3的批量，标签的预测值和真实值如下表所示：

| $\hat{y}$ | $y$ |
| --- | --- |
| 2.33 | 3.14 |
| 1.07 | 0.98 |
| 1.23 | 1.32 |

求该批量的平均损失函数
```
import torch

y_hat = torch.tensor([2.33, 1.07, 1.23])
y = torch.tensor([3.14, 0.98, 1.32])

def Loss(y_hat, y):
    return (y_hat - y) ** 2 / 2

# tensor.sum()函数将tensor元素累加
mean = Loss(y_hat, y).sum() / 3

print('%.3f' % mean)
```
