## 数据操作
### 1.view( )函数
- 用 **`view()`** 来改变 **`Tensor`** 的形状：

  - <code>**view(3,2)**</code>：转化为3行2列
 
  - <code>**view(3,-1)**</code>：-1所指的维度可以根据其他维度推算出来，例如此处共6个元素，此时-1就代表2.
 
---
 
### 2.np.random.normal(loc, scale, size)
 - 生成正态分布的概率密度随机数
   - **loc：** 此概率分布的均值
   - **scale：** 此概率分布的标准差，越大越矮胖，越小越瘦高
   - **size：** 输出的 **`shape`** ，默认为None,只输出一个值
```
nd1 = np.random.normal(loc=1,scale=2,size=2)
#array([-0.46982446, -1.28956852])
```

---

### 3.init.normal_(tensor, mean, std)
- 通过Pytorch提供的**`init`**模块，实现正态分布
  - **tensor：** 输入的张量
  - **mean：** 均值
  - **std：** 标准差
```
init.normal_(net[0].weight, mean=0, std=0.01)
# 将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。
```

---

### 4.练习

在线性回归模型中，对于某个大小为3的批量，标签的预测值和真实值如下表所示：

| $\hat{y}$ | $y$ |
| --- | --- |
| 2.33 | 3.14 |
| 1.07 | 0.98 |
| 1.23 | 1.32 |

求该批量的平均损失函数
```
import torch

y_hat = torch.tensor([2.33, 1.07, 1.23])
y = torch.tensor([3.14, 0.98, 1.32])

def Loss(y_hat, y):
    return (y_hat - y) ** 2 / 2

# tensor.sum()函数将tensor元素累加
mean = Loss(y_hat, y).sum() / 3

print('%.3f' % mean)
```

---
### 5.PyTorch中的广播机制
#### 一般语义
在PyTorch中，若以下条件成立，则两个张量可进行广播
 - 每个张量具有至上一个维度
 - 在遍历维度大小时，从尾部开始遍历，二者维度必须相等；或其中一个要么是1，要么不存在

 例：

```
x = torch.tensor(5, 3, 4, 1)
y = torch,tensor(   3, 1, 1)

>>> x.size = ([5, 3, 4, 1])
>>> y.size = ([3, 1, 1])
```
此时x与y可以进行广播，因为从尾部进行遍历

||4th|3rd|2nd|1st|
|:-:|:-:|:-:|:-:|:-:|
|x|5|3|4|1|
|y| |3|1|1|

```
1st中，1=1，二者维度相等，满足条件
2nd中，4!=1，但y的维度为1，满足条件
3rd中，3=3，二者维度相等，满足条件
4th中，y的对应维度为空，满足条件
```
而当如下情况时，无法进行广播

```
x = torch.tensor(5, 3, 4, 1)
y = torch,tensor(   2, 1, 1)

>>> x.size = ([5, 3, 4, 1])
>>> y.size = ([2, 1, 1])
```

||4th|3rd|2nd|1st|
|:-:|:-:|:-:|:-:|:-:|
|x|5|3|4|1|
|y| |2|1|1|

```
1st中，1=1，二者维度相等，满足条件
2nd中，4!=1，但y的维度为1，满足条件
3rd中，3!=2，二者维度不相等，也不存在维度为1，不满足条件
无法广播
```

#### inplace操作中不允许使用广播
`inplace`操作指就地操作，不允许使用临时变量

PyTorch操作`inplace`版本都有后缀 `_` , 例如`x.add_(y)`是`inplace`操作，而`torch.add(x, y)`则不是。